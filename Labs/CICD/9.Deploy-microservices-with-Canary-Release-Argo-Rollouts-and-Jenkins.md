This project outlines an **Enterprise-grade CI/CD pipeline for Microservices**, incorporating **Canary Releases** using **Argo Rollouts** and automated testing with **Jenkins**. The core principle is GitOps, where your Git repository is the single source of truth for all application and infrastructure configurations.

You will typically have **two main Git repositories** for this setup:

1.  **`my-microservice-a-src`**: This repository holds the source code for a specific microservice (e.g., Microservice A), its `Dockerfile`, unit/integration tests, and the `Jenkinsfile` that defines its CI pipeline.
2.  **`my-microservices-k8s-manifests`**: This central GitOps repository contains all Kubernetes deployment and service manifests for *all* your microservices, organized by environment (staging, production) and using Helm charts for templating. ArgoCD will monitor this repository.

### Requirement

ðŸŒ³ Level 3: Advanced (Enterprise-grade)
âœ… Project name: Deploy microservices with Canary Release, Argo Rollouts and Jenkins
Goal:

Complete CI/CD for multiple microservices.

Use Argo Rollouts to deploy Canary deployment.

Jenkins build and test each service, update version separately.

Automatic rollback if errors are detected during canary.

Technical knowledge:

Jenkins matrix pipelines

Helm chart/Kustomize + standard GitOps folder structure

ArgoCD + Argo Rollouts

Metrics monitoring (Prometheus, Grafana)

Overview workflow:

Dev push code to microservice A â†’ Jenkins build/test/deploy â†’ push YAML to Git.

ArgoCD detect update â†’ use Argo Rollouts to canary deploy (10% â†’ 50% â†’ 100%).

Monitor error/response speed metrics (via Prometheus).

If error detected â†’ automatic rollback.

If OK â†’ promote to 100%.

-----

## Project: Microservices Deployment with Canary Release, Argo Rollouts, and Jenkins

**Goal:**
To implement a sophisticated CI/CD pipeline for multiple microservices, featuring:

  * **Full CI/CD for Multiple Microservices**: Each service has its own build, test, and deployment pipeline.
  * **Canary Deployment with Argo Rollouts**: Gradually roll out new versions of microservices to a small percentage of users before a full rollout.
  * **Jenkins Build & Test**: Jenkins handles building Docker images, running comprehensive tests (unit, integration), and updating specific service versions in the GitOps repository.
  * **Automated Rollback**: Argo Rollouts automatically rolls back to the previous stable version if predefined error metrics or response times exceed thresholds during the canary analysis.

**Key Technologies:**

  * **Jenkins**: Orchestrates CI (build, test, Docker image push) and updates GitOps manifests.
  * **Docker Hub (or Amazon ECR / Google Container Registry)**: Centralized Docker image registry.
  * **GitHub**: Central version control for both application source code and Kubernetes manifests (GitOps repo).
  * **Helm**: Used for templating Kubernetes manifests, allowing easy environment separation and configuration.
  * **ArgoCD**: The GitOps continuous delivery tool, syncing the desired state from the GitOps repository to Kubernetes.
  * **Argo Rollouts**: An extension for Kubernetes that provides advanced deployment strategies like Canary, Blue/Green, and automated rollbacks based on metrics.
  * **Prometheus & Grafana**: (Assumed to be present) For collecting and visualizing application metrics, which Argo Rollouts uses for its analysis steps.

### Overall Workflow:

1.  **Developer Pushes Code to Microservice A (e.g., `dev` branch):**
      * A developer commits changes to `my-microservice-a-src` (e.g., to the `dev` branch).
2.  **Jenkins Build/Test/Deploy to Staging:**
      * Jenkins detects the push, triggers the `Jenkinsfile` for Microservice A.
      * **Build & Test**: Jenkins builds the Docker image for Microservice A, runs unit and integration tests.
      * **Push Image**: If tests pass, Jenkins pushes the new Docker image to Docker Hub.
      * **Update GitOps Manifests**: Jenkins then updates the image tag for Microservice A within the `values.yaml` file in the `environments/staging/microservice-a/values.yaml` path of the `my-microservices-k8s-manifests` GitOps repository. It commits and pushes this change.
3.  **ArgoCD Detects Update & Triggers Canary (Staging):**
      * ArgoCD, monitoring `my-microservices-k8s-manifests`, detects the change in `staging/microservice-a/values.yaml`.
      * It triggers an **Argo Rollout** for Microservice A in the Kubernetes staging cluster.
      * **Canary Deployment**: Argo Rollouts begins the canary process (e.g., 10% new pods, then 50%, then 100%).
      * **Metrics Monitoring (via Prometheus)**: During each step of the canary, Argo Rollouts queries Prometheus for predefined metrics (e.g., error rates, latency).
      * **Automated Rollback**: If metrics indicate a failure (e.g., error rate spikes above a threshold), Argo Rollouts automatically rolls back to the previous stable version.
      * **Promotion**: If all analysis steps pass, Argo Rollouts promotes the new version to 100%.
4.  **QA Test & Merge to Main:**
      * Once QA tests are successful in the staging environment, the `dev` branch of `my-microservice-a-src` is merged into the `main` branch.
5.  **Jenkins Build/Test/Deploy to Production (via `main` branch):**
      * Jenkins detects the merge to `main`, runs the same build/test steps.
      * It pushes the "production-ready" Docker image (potentially with a `latest` tag as well).
      * It updates the image tag for Microservice A in `environments/production/microservice-a/values.yaml` in the GitOps repository.
6.  **ArgoCD Detects Update & Triggers Canary (Production):**
      * ArgoCD detects the change in `production/microservice-a/values.yaml`.
      * It triggers an **Argo Rollout** for Microservice A in the Kubernetes production cluster, following the same canary deployment and automated rollback process.

-----

## 1\. Microservice Source Code Repository (`my-microservice-a-src`)

Create a GitHub repository named `my-microservice-a-src`. This will be a typical structure for one of your microservices.

### `app.js`

A simple Node.js HTTP server for Microservice A.

```javascript
// app.js
const http = require('http');

const hostname = '0.0.0.0';
const port = 8080;
const serviceName = process.env.SERVICE_NAME || 'Microservice A';
const appVersion = process.env.APP_VERSION || '1.0';

const server = http.createServer((req, res) => {
  res.statusCode = 200;
  res.setHeader('Content-Type', 'text/plain');
  res.end(`Hello from ${serviceName}! Version: ${appVersion}\n`);
});

server.listen(port, hostname, () => {
  console.log(`Server ${serviceName} running at http://${hostname}:${port}/`);
});
```

### `package.json`

Basic package definition for Microservice A.

```json
{
  "name": "microservice-a",
  "version": "1.0.0",
  "description": "Microservice A for enterprise-grade CI/CD.",
  "main": "app.js",
  "scripts": {
    "start": "node app.js",
    "test": "node tests/run_tests.js"
  },
  "author": "Your Name",
  "license": "ISC",
  "devDependencies": {
    "axios": "^1.6.8"
  }
}
```

### `Dockerfile`

Defines how to build the Docker image for Microservice A.

```dockerfile
# Dockerfile for Microservice A
FROM node:18-alpine

WORKDIR /usr/src/app

COPY package*.json ./
RUN npm install --production

COPY . .

EXPOSE 8080

CMD [ "node", "app.js" ]
```

### `tests/run_tests.js`

Basic unit/integration tests for Microservice A.

```javascript
// tests/run_tests.js
const http = require('http');
const axios = require('axios');

console.log('Running unit tests for Microservice A...');

// --- Unit Test Example ---
function runUnitTest() {
  const testMessage = 'Hello from Microservice A!';
  if (typeof testMessage === 'string') {
    console.log('Microservice A Unit Test Passed: Message is a string.');
  } else {
    console.error('Microservice A Unit Test Failed: Message is not a string.');
    process.exit(1);
  }
}

// --- Integration Test Example ---
// This test would typically hit a temporarily deployed instance or a mocked service.
// For simplicity, this example simulates a local server for the test.
function runIntegrationTest() {
  console.log('Running integration test for Microservice A...');
  const testPort = 8080;
  const testHostname = 'localhost';

  const server = http.createServer((req, res) => {
    res.statusCode = 200;
    res.setHeader('Content-Type', 'text/plain');
    res.end('Test response from dummy server for Microservice A\n');
  });

  server.listen(testPort, testHostname, async () => {
    try {
      const response = await axios.get(`http://${testHostname}:${testPort}/`);
      if (response.status === 200 && response.data.includes('Test response')) {
        console.log('Microservice A Integration Test Passed: Received expected response.');
      } else {
        console.error('Microservice A Integration Test Failed: Unexpected response.');
        process.exit(1);
      }
    } catch (error) {
      console.error('Microservice A Integration Test Failed:', error.message);
      process.exit(1);
    } finally {
      server.close();
    }
  });
}

// Execute tests
runUnitTest();
runIntegrationTest(); // In a real CI, this might hit a temporary container
console.log('All tests for Microservice A completed.');
```

### `Jenkinsfile`

This `Jenkinsfile` will be placed in the root of `my-microservice-a-src`. It uses a multibranch pipeline approach.
**Remember to replace placeholders**: `YOUR_DOCKERHUB_USERNAME`, `YOUR_K8S_MANIFESTS_REPO_URL`, `YOUR_K8S_MANIFESTS_REPO_CREDENTIALS_ID`.

```groovy
// Jenkinsfile for Microservice A
pipeline {
    agent any

    environment {
        SERVICE_NAME = "microservice-a"
        DOCKER_IMAGE_NAME = "YOUR_DOCKERHUB_USERNAME/${SERVICE_NAME}" // e.g., myuser/microservice-a
        K8S_MANIFESTS_REPO = "YOUR_K8S_MANIFESTS_REPO_URL" // e.g., git@github.com:youruser/my-microservices-k8s-manifests.git
        K8S_MANIFESTS_CREDENTIALS_ID = "YOUR_K8S_MANIFESTS_REPO_CREDENTIALS_ID" // Jenkins secret text credential ID for K8s repo access
    }

    stages {
        stage('Checkout Source Code') {
            steps {
                echo "Checking out ${SERVICE_NAME} source code..."
                checkout scm
            }
        }

        stage('Install Dependencies for Tests') {
            steps {
                sh 'npm install'
            }
        }

        stage('Run Unit and Integration Tests') {
            steps {
                sh 'npm test'
            }
        }

        stage('Build Docker Image') {
            steps {
                script {
                    def imageTag = "${env.BUILD_NUMBER}" // Use Jenkins build number as tag
                    sh "docker build -t ${DOCKER_IMAGE_NAME}:${imageTag} ."
                    env.DOCKER_IMAGE_TAG = imageTag // Store the tag for later use
                }
            }
        }

        stage('Push Docker Image') {
            steps {
                script {
                    withCredentials([usernamePassword(credentialsId: 'dockerhub-credentials', passwordVariable: 'DOCKER_PASSWORD', usernameVariable: 'DOCKER_USERNAME')]) {
                        sh "echo ${DOCKER_PASSWORD} | docker login --username ${DOCKER_USERNAME} --password-stdin"
                        sh "docker push ${DOCKER_IMAGE_NAME}:${env.DOCKER_IMAGE_TAG}"
                        // Only push 'latest' for production builds (main branch)
                        if (env.BRANCH_NAME == 'main') {
                            sh "docker tag ${DOCKER_IMAGE_NAME}:${env.DOCKER_IMAGE_TAG} ${DOCKER_IMAGE_NAME}:latest"
                            sh "docker push ${DOCKER_IMAGE_NAME}:latest"
                        }
                        sh "docker logout"
                    }
                }
            }
        }

        stage('Update Kubernetes Manifests') {
            steps {
                script {
                    dir('k8s_repo') { // Clone K8s manifests into a temporary directory
                        git branch: 'main', credentialsId: "${K8S_MANIFESTS_CREDENTIALS_ID}", url: "${K8S_MANIFESTS_REPO}"

                        def valuesFilePath
                        def commitMessagePrefix

                        if (env.BRANCH_NAME == 'dev') {
                            valuesFilePath = "environments/staging/${SERVICE_NAME}/values.yaml"
                            commitMessagePrefix = "Update staging ${SERVICE_NAME} image"
                        } else if (env.BRANCH_NAME == 'main') {
                            valuesFilePath = "environments/production/${SERVICE_NAME}/values.yaml"
                            commitMessagePrefix = "Update production ${SERVICE_NAME} image"
                        } else {
                            error "Unsupported branch: ${env.BRANCH_NAME}. Only 'dev' and 'main' are supported for deployment."
                        }

                        // Use yq to update the image tag in the Helm values file
                        // Install yq on your Jenkins agent if not already present
                        sh "yq e '.image.tag = \"${env.DOCKER_IMAGE_TAG}\"' -i ${valuesFilePath}"
                        sh "yq e '.image.repository = \"${DOCKER_IMAGE_NAME}\"' -i ${valuesFilePath}" // Ensure repository is also correct

                        // Git configuration and push
                        sh "git config user.email 'jenkins@example.com'"
                        sh "git config user.name 'Jenkins CI'"
                        sh "git add ${valuesFilePath}"
                        sh "git commit -m '${commitMessagePrefix} to ${env.DOCKER_IMAGE_TAG} [skip ci]'" // [skip ci] to prevent infinite loop
                        sh "git push origin main"
                    }
                }
            }
        }
    }
    post {
        always {
            cleanWs() // Clean up workspace
        }
        failure {
            echo "Pipeline failed. Check logs for details."
            // Add notifications (Slack, Email, etc.)
        }
        success {
            echo "Pipeline completed successfully!"
        }
    }
}
```

-----

## 2\. Kubernetes Manifests Repository (`my-microservices-k8s-manifests`)

Create a GitHub repository named `my-microservices-k8s-manifests`. This repository will contain your Helm charts and ArgoCD Application definitions.

### Helm Chart for Microservice A (`charts/microservice-a/`)

Create a `charts` directory, and inside it, a `microservice-a` directory. This will be a standard Helm chart structure.

```
my-microservices-k8s-manifests/
â”œâ”€â”€ charts/
â”‚   â””â”€â”€ microservice-a/
â”‚       â”œâ”€â”€ Chart.yaml
â”‚       â”œâ”€â”€ templates/
â”‚       â”‚   â”œâ”€â”€ _helpers.tpl
â”‚       â”‚   â”œâ”€â”€ rollout.yaml
â”‚       â”‚   â””â”€â”€ service.yaml
â”‚       â””â”€â”€ values.yaml
â””â”€â”€ environments/
    â”œâ”€â”€ staging/
    â”‚   â””â”€â”€ microservice-a/
    â”‚       â””â”€â”€ values.yaml
    â””â”€â”€ production/
        â””â”€â”€ microservice-a/
            â””â”€â”€ values.yaml
```

#### `charts/microservice-a/Chart.yaml`

```yaml
# charts/microservice-a/Chart.yaml
apiVersion: v2
name: microservice-a
description: A Helm chart for Microservice A
version: 0.1.0
appVersion: "1.0.0"
```

#### `charts/microservice-a/templates/rollout.yaml`

This defines the Argo Rollout for Microservice A, including the Canary strategy and analysis.
**Important**: Replace `YOUR_DOCKERHUB_USERNAME` in `values.yaml` later, not here.

```yaml
# charts/microservice-a/templates/rollout.yaml
apiVersion: argoproj.io/v1alpha1
kind: Rollout
metadata:
  name: {{ include "microservice-a.fullname" . }}
  labels:
    {{- include "microservice-a.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      {{- include "microservice-a.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "microservice-a.selectorLabels" . | nindent 8 }}
    spec:
      containers:
      - name: {{ .Chart.Name }}
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        ports:
        - containerPort: {{ .Values.service.targetPort }}
        env:
        - name: SERVICE_NAME
          value: "{{ .Chart.Name }}"
        - name: APP_VERSION
          value: "{{ .Values.image.tag }}"
        resources:
          {{- toYaml .Values.resources | nindent 10 }}
  strategy:
    canary:
      steps:
      - setWeight: 10 # Send 10% traffic to new version
      - pause: {} # Manual intervention point (or remove for full automation)
      - analysis:
          templates:
          - templateName: {{ .Chart.Name }}-success-rate
          - templateName: {{ .Chart.Name }}-latency
          args:
          - name: service-name
            value: {{ include "microservice-a.fullname" . }}
      - setWeight: 50 # Send 50% traffic to new version
      - pause: { duration: 30s } # Wait for 30 seconds
      - analysis:
          templates:
          - templateName: {{ .Chart.Name }}-success-rate
          - templateName: {{ .Chart.Name }}-latency
          args:
          - name: service-name
            value: {{ include "microservice-a.fullname" . }}
      - setWeight: 100 # Promote to 100%
```

#### `charts/microservice-a/templates/service.yaml`

```yaml
# charts/microservice-a/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: {{ include "microservice-a.fullname" . }}
  labels:
    {{- include "microservice-a.labels" . | nindent 4 }}
spec:
  type: {{ .Values.service.type }}
  ports:
    - port: {{ .Values.service.port }}
      targetPort: {{ .Values.service.targetPort }}
      protocol: TCP
      name: http
  selector:
    {{- include "microservice-a.selectorLabels" . | nindent 4 }}
```

#### `charts/microservice-a/templates/_helpers.tpl`

Standard Helm helper file.

```yaml
{{/*
Expand the name of the chart.
*/}}
{{- define "microservice-a.name" -}}
{{- default .Chart.Name .Values.nameOverride -}}
{{- end -}}

{{/*
Create a default fully qualified app name.
We truncate at 63 chars because some Kubernetes name fields are limited to this (by the DNS naming spec).
If release name contains chart name it will be used as a full name.
*/}}
{{- define "microservice-a.fullname" -}}
{{- if .Values.fullnameOverride -}}
{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" -}}
{{- else -}}
{{- $name := default .Chart.Name .Values.nameOverride -}}
{{- if contains $name .Release.Name -}}
{{- .Release.Name | trunc 63 | trimSuffix "-" -}}
{{- else -}}
{{- printf "%s-%s" .Release.Name $name | trunc 63 | trimSuffix "-" -}}
{{- end -}}
{{- end -}}
{{- end -}}

{{/*
Create chart name and version as part of the labels
*/}}
{{- define "microservice-a.labels" -}}
helm.sh/chart: {{ include "microservice-a.chart" . }}
{{ include "microservice-a.selectorLabels" . }}
{{- if .Chart.AppVersion }}
app.kubernetes.io/version: {{ .Chart.AppVersion | quote }}
{{- end }}
app.kubernetes.io/managed-by: {{ .Release.Service }}
{{- end -}}

{{/*
Selector labels
*/}}
{{- define "microservice-a.selectorLabels" -}}
app.kubernetes.io/name: {{ include "microservice-a.name" . }}
app.kubernetes.io/instance: {{ .Release.Name }}
{{- end -}}

{{/*
Create the name of the service account to use
*/}}
{{- define "microservice-a.serviceAccountName" -}}
{{- if .Values.serviceAccount.create -}}
    {{- default (include "microservice-a.fullname" .) .Values.serviceAccount.name -}}
{{- else -}}
    {{- default "default" .Values.serviceAccount.name -}}
{{- end -}}
{{- end -}}

{{/*
Chart version
*/}}
{{- define "microservice-a.chart" -}}
{{- printf "%s-%s" .Chart.Name .Chart.Version | replace "+" "_" | trunc 63 | trimSuffix "-" -}}
{{- end -}}
```

#### `charts/microservice-a/values.yaml` (Default values for Helm chart)

```yaml
# charts/microservice-a/values.yaml
replicaCount: 1

image:
  repository: YOUR_DOCKERHUB_USERNAME/microservice-a # This will be updated by Jenkins
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "1.0" # This will be updated by Jenkins

service:
  type: ClusterIP # Change to LoadBalancer for external access if needed in prod
  port: 80
  targetPort: 8080

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts will run on environments with little
  # resources, such as Minikube.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi
```

### Environment-Specific Values (`environments/`)

These `values.yaml` files will override the default Helm chart values for specific environments. Jenkins will update the `image.tag` in these files.

#### `environments/staging/microservice-a/values.yaml`

```yaml
# environments/staging/microservice-a/values.yaml
replicaCount: 1 # Staging might have fewer replicas

image:
  repository: YOUR_DOCKERHUB_USERNAME/microservice-a
  tag: "1.0" # Jenkins will update this

service:
  type: LoadBalancer # Expose staging externally for QA

# Add any staging-specific configurations here
```

#### `environments/production/microservice-a/values.yaml`

```yaml
# environments/production/microservice-a/values.yaml
replicaCount: 3 # Production typically has more replicas

image:
  repository: YOUR_DOCKERHUB_USERNAME/microservice-a
  tag: "1.0" # Jenkins will update this

service:
  type: LoadBalancer # Expose production externally

# Add any production-specific configurations here
```

### Argo Rollouts Analysis Templates (`argocd-rollouts-analysis-templates.yaml`)

These `AnalysisTemplate` resources define the metrics Argo Rollouts will use to determine the success or failure of a canary step. These need to be applied to your Kubernetes cluster once.
**Assumes Prometheus is installed and scraping metrics.**

```yaml
# argocd-rollouts-analysis-templates.yaml
apiVersion: argoproj.io/v1alpha1
kind: AnalysisTemplate
metadata:
  name: microservice-a-success-rate
spec:
  args:
  - name: service-name
  metrics:
  - name: success-rate
    interval: 10s
    successCondition: result[0] >= 0.99 # 99% success rate
    failureCondition: result[0] < 0.95 # Below 95% success rate
    # Prometheus query: (sum(rate(http_requests_total{job="my-app", service_name="{{args.service-name}}", status_code=~"2.."}[1m])) / sum(rate(http_requests_total{job="my-app", service_name="{{args.service-name}}}[1m])))
    # Replace 'job="my-app"' and 'service_name' with your actual Prometheus labels
    # This example assumes you have an http_requests_total metric from your application.
    # You might need to instrument your Node.js app with a Prometheus client.
    # For a simple test, you could use a dummy metric or just rely on manual pause.
    prometheus:
      address: http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090 # Adjust to your Prometheus service
      query: |
        sum(rate(http_requests_total{job="my-app", service_name="{{args.service-name}}", status_code=~"2.."}[1m])) / sum(rate(http_requests_total{job="my-app", service_name="{{args.service-name}}}[1m]))
---
apiVersion: argoproj.io/v1alpha1
kind: AnalysisTemplate
metadata:
  name: microservice-a-latency
spec:
  args:
  - name: service-name
  metrics:
  - name: latency-p99
    interval: 10s
    successCondition: result[0] < 0.5 # p99 latency less than 0.5 seconds
    failureCondition: result[0] > 1.0 # p99 latency greater than 1.0 seconds
    # Prometheus query: histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket{job="my-app", service_name="{{args.service-name}}}[1m])) by (le))
    # Adjust to your Prometheus service
    prometheus:
      address: http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090 # Adjust to your Prometheus service
      query: |
        histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket{job="my-app", service_name="{{args.service-name}}}[1m])) by (le))
```

-----

## Setup Instructions

### Prerequisites:

  * A running **Kubernetes cluster**.
  * **Jenkins** installed and configured. Ensure Jenkins has:
      * Docker installed and configured (for building images).
      * Git plugin installed.
      * `yq` installed on the Jenkins agent (used in `Jenkinsfile` to modify YAML).
  * **ArgoCD** installed in your Kubernetes cluster.
  * **Argo Rollouts** installed in your Kubernetes cluster.
  * **Prometheus** installed and configured to scrape metrics from your applications within Kubernetes.
  * **Docker Hub** account (or equivalent registry).
  * **GitHub** accounts for both repositories.

### 1\. Jenkins Configuration:

  * **Docker Hub Credentials**:
      * In Jenkins, go to `Manage Jenkins` \> `Manage Credentials` \> `(global)` \> `Add Credentials`.
      * **Kind**: "Username with password"
      * **Scope**: Global
      * **Username**: Your Docker Hub username
      * **Password**: Your Docker Hub password
      * **ID**: `dockerhub-credentials` (This ID is referenced in `Jenkinsfile`)
  * **Kubernetes Manifests Repo Credentials**:
      * If your `my-microservices-k8s-manifests` repo is private, add a Git credential (e.g., "SSH Username with private key" or "Username with password") with an **ID** matching `YOUR_K8S_MANIFESTS_REPO_CREDENTIALS_ID` in your `Jenkinsfile`. This credential needs push access to the `my-microservices-k8s-manifests` repository.
  * **Create a Jenkins Multibranch Pipeline Job**:
      * Go to Jenkins dashboard \> `New Item`.
      * Enter an item name (e.g., `Microservice-A-CI-CD`).
      * Select "Multibranch Pipeline" and click `OK`.
      * In the job configuration:
          * **Branch Sources**: Click `Add source` \> `Git`.
          * **Project Repository**: `https://github.com/YOUR_GITHUB_USERNAME/my-microservice-a-src.git` (or SSH URL).
          * **Credentials**: Select credentials to access `my-microservice-a-src` (if it's private).
          * **Build Strategies**: "Default" is usually fine.
          * **Discover Branches**: "Filter by name (with regular expression)".
              * **Include**: `(dev|main)` (This will discover and build `dev` and `main` branches).
          * **Build Configuration**:
              * **Mode**: `by Jenkinsfile` (default).
              * **Script Path**: `Jenkinsfile` (default).
      * Click `Save`.
      * Jenkins will now scan your repository and automatically create pipeline jobs for `dev` and `main` branches.

### 2\. Kubernetes Cluster Setup (Initial):

  * **Apply Argo Rollouts Analysis Templates**:
      * Apply the `argocd-rollouts-analysis-templates.yaml` to your cluster. These templates are global.
      * `kubectl apply -f argocd-rollouts-analysis-templates.yaml`

### 3\. ArgoCD Configuration:

Access your ArgoCD UI (typically via `kubectl port-forward svc/argocd-server -n argocd 8080:443` or your ingress setup).

  * **Log in** (default: admin/password from initial setup).

#### Create ArgoCD Application for Staging Microservice A:

  * Click `New App` or the `+` icon.
  * **Application Name**: `microservice-a-staging`
  * **Project**: `default` (or your custom project)
  * **Sync Policy**: Select `Automatic` and check `PRUNE RESOURCES` and `SELF HEAL`.
  * **Source**:
      * **Repository URL**: `https://github.com/YOUR_GITHUB_USERNAME/my-microservices-k8s-manifests.git` (or SSH URL)
      * **Revision**: `HEAD` (or `main` branch, as Jenkins pushes to `main` for all environment paths)
      * **Path**: `environments/staging/microservice-a` (This points to the Helm chart *instance* for staging)
      * **Helm**: Check `Helm` for the manifest type.
  * **Destination**:
      * **Cluster**: `in-cluster` (or the name of your registered cluster)
      * **Namespace**: `default` (or your desired staging namespace, e.g., `my-app-staging`)
  * Click `Create`.

#### Create ArgoCD Application for Production Microservice A:

  * Click `New App` or the `+` icon.
  * **Application Name**: `microservice-a-production`
  * **Project**: `default` (or your custom project)
  * **Sync Policy**: Select `Automatic` and check `PRUNE RESOURCES` and `SELF HEAL`.
  * **Source**:
      * **Repository URL**: `https://github.com/YOUR_GITHUB_USERNAME/my-microservices-k8s-manifests.git` (or SSH URL)
      * **Revision**: `HEAD` (or `main` branch)
      * **Path**: `environments/production/microservice-a`
      * **Helm**: Check `Helm`.
  * **Destination**:
      * **Cluster**: `in-cluster` (or the name of your registered cluster)
      * **Namespace**: `default` (or your desired production namespace, e.g., `my-app-prod`)
  * Click `Create`.

-----

## Workflow and Testing:

### 1\. Initial Deployment (Staging):

  * Make an initial commit to the `dev` branch of `my-microservice-a-src`.
  * **Jenkins**: Jenkins will detect the push to `dev`, run tests, build the Docker image, push it, and then update `environments/staging/microservice-a/values.yaml` in `my-microservices-k8s-manifests`.
  * **ArgoCD (Staging App)**: ArgoCD will detect the change in `staging/microservice-a/values.yaml` and automatically sync. This will trigger an **Argo Rollout** for Microservice A in your staging cluster.
      * Monitor the Rollout in the ArgoCD UI or using `kubectl argo rollouts get rollout microservice-a-staging-microservice-a -n <your-namespace>`.
      * You'll see it progress through the canary steps (10%, pause, analysis, 50%, pause, analysis, 100%).
      * If you included `pause: {}` in the Rollout steps, you'll need to manually promote it (`kubectl argo rollouts promote microservice-a-staging-microservice-a -n <your-namespace>`).
  * **Verify Staging**: Get the LoadBalancer IP for `microservice-a-staging-service` and access it. You should see "Hello from Microservice A\! Version: \<build-number\>".

### 2\. QA Testing and Promotion to Production:

  * Once the application is tested and verified in the staging environment by QA:
  * **Merge `dev` into `main`**: Create a Pull Request from `dev` to `main` in your `my-microservice-a-src` repository and merge it.
  * **Jenkins (Main Branch)**: Jenkins will detect the merge to `main`, run tests again, build the Docker image, push it (and the `latest` tag), and then update `environments/production/microservice-a/values.yaml` in `my-microservices-k8s-manifests`.
  * **ArgoCD (Production App)**: ArgoCD will detect the change in `production/microservice-a/values.yaml` and automatically sync. This will trigger an **Argo Rollout** for Microservice A in your production cluster.
      * Monitor the Rollout's progress. If the Prometheus metrics (success rate, latency) indicate issues, Argo Rollouts will automatically perform a rollback.
      * If all analysis passes, the new version will be promoted to 100%.
  * **Verify Production**: Get the LoadBalancer IP for `microservice-a-production-service` and access it. You should see the new version.

This enterprise-grade setup provides a robust, automated, and GitOps-driven CI/CD pipeline, ensuring safe and reliable deployments of your microservices with advanced strategies like canary releases and automated rollbacks.